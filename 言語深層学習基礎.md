# トピックモデル
* 比較的単純なモデルを扱う
* パラメータ数が人間に把握できる範囲のもの
* 人間に理解しやすい
* テキストマイニング等に威力を発揮

### Unigram(ユニグラム)モデル
* 文の中で、各単語は、独立した存在
* 仮定:何もないところから、順次、確率に従って単語が生成されていく。
* P(d) = P(w1)・P(w2)・...・P(wn)

* モデルの例
  * 「今日は、友人とサッカーをした」 P(今日)・P(は)・P(友人)・P(と)・P(サッカー)・P(を)・P(した) =0.05 ・0.1・0.002・0.1・0.001・0.2・0.05
  * Unigramモデルによれば、「今日は、友人とサッカーをした」という文が 生成される確率は、1e-11 (=1/100000000000)

### Bigramモデル
* 前の単語から次の単語の確率を求めるモデル
### IBMモデル
* フランス語と英語の文のペアの生成確率を求める

## トピック 
* この文章は何について語っている文章か？ということを示す 
* オフサイドはサッカーに関する話をしている

### Unigram Mixture
* トピックを使用した確率モデルの例
* 文章にトピック番号tを付加する。各話題によって単語の出現率が変わる

## 自然言語でのモデル
* 確立的 識別モデル
  * 単語の重要度を頼りに計算する
  * データを与えられた時のラベルの確立を直接計算する
* 確立的 生成モデル
  * 全体の文を生成する必要があり 文がどのようにできるのかの生成過程をモデル化したもの
  * ラベルが与えられたときの、データの生成確立を計算
* ベイズの定理
  * P(y)が与えられる、 P(x|y)からP(y|x)を求める。
  * P(y|x) = P(y)P(x|y) / P(x)
