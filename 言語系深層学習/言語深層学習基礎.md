# トピックモデル
* 比較的単純なモデルを扱う
* パラメータ数が人間に把握できる範囲のもの
* 人間に理解しやすい
* テキストマイニング等に威力を発揮

### Unigram(ユニグラム)モデル
* 文の中で、各単語は、独立した存在
* 仮定:何もないところから、順次、確率に従って単語が生成されていく。
* P(d) = P(w1)・P(w2)・...・P(wn)

* モデルの例
  * 「今日は、友人とサッカーをした」 P(今日)・P(は)・P(友人)・P(と)・P(サッカー)・P(を)・P(した) =0.05 ・0.1・0.002・0.1・0.001・0.2・0.05
  * Unigramモデルによれば、「今日は、友人とサッカーをした」という文が 生成される確率は、1e-11 (=1/100000000000)

### Bigramモデル
* 前の単語から次の単語の確率を求めるモデル
### IBMモデル
* フランス語と英語の文のペアの生成確率を求める

## トピック 
* この文章は何について語っている文章か？ということを示す 
* オフサイドはサッカーに関する話をしている

### Unigram Mixture
* トピックを使用した確率モデルの例
* 文章にトピック番号tを付加する。各話題によって単語の出現率が変わる

## 自然言語でのモデル
* 確立的 識別モデル
  * 単語の重要度を頼りに計算する
  * データを与えられた時のラベルの確立を直接計算する
* 確立的 生成モデル
  * 全体の文を生成する必要があり 文がどのようにできるのかの生成過程をモデル化したもの
  * ラベルが与えられたときの、データの生成確立を計算
* ベイズの定理
  * P(y)が与えられる、 P(x|y)からP(y|x)を求める。
  * P(y|x) = P(y)P(x|y) / P(x)

# BERT(Bidirectional Encoder Representations from Transformers)
## 自然言語処理モデル（BERT）を利用した日本語の文章分類
* 文脈を読むことが可能になった
* Transformerというアーキテクチャ（構造）が組み込まれており、文章を双方向（文頭と文末）から学習することによって「文脈を読むこと」が実現
* 既存のタスク処理モデルに転移学習し導入
* 文脈理解
* 汎用性
* データ不足の克服

## 自然言語処理（Natural Language Processing）
* 言葉が持つ意味を解析する処理
* 単語を高次元のベクトルに置き換える分散表現という技術を用いて入力
* 単語データの並びのことを「シーケンス」
* 事前学習モデルであり、入力されたラベルが付与されていない、つまり名前がついていない分散表現をTransformerが処理することによって学習します。実際には、TransformerがMasked Language ModelとNext Sentence Predictionという2つの手法を同時進行で行うことで学習
* 双方向のTransformerによって学習するため、従来の手法に比べ精度が向上

## Masked Language Model
* 入力文の15%の単語を別の単語を置き換えて文脈から置き換える前の単語を予測させる
* 80%はMASKとして変換して置換された単語を周りの文脈から当てるタスクを解くことで単語に対応する文脈情報を学習

## Next Sentence Prediction
* 2つの入力文に対して「その2文が隣り合っているか」を当てるよう学習
* 文の片方を50%の確率で他の文に置き換え、それらが隣り合っているか（isNext）隣り合っていない（notNext）か判別することによって学習
* 単語だけでなく文全体の表現についても学習

## 行ったタスク
* MNLI：含意関係の分類タスク
* QQP：質問内容が同じであるかを分類するタスク
* QNLI：質問と文が与えられ文が質問の答えになるか当てる分類タスク
* SST-2：映画のレビューに対する感情分析タスク
* CoLA：文の文法性判断を行う分類タスク
* STS-B：2文の類似度を5段階で評価する分類タスク
* MRPC：ニュースに含まれる2文の意味が等しいかを当てる分類タスク
* RTE：小規模な含意関係の分類タスク

## 課題
* 計算する際のパラメーターが多く、層が厚い「巨大なモデル」

## 参考URL
* https://ledge.ai/bert/

* googleclabで動かす
  * https://qiita.com/takubb/items/fd972f0ac3dba909c293#3-学習済みモデルのロード
  * エラー対策
    * https://qiita.com/Afo_guard_enthusiast/items/88ce7d9610073ac1eca2
